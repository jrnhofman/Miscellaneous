{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are trying to use Keras neural networks to solve a classification problem. This is an adapted dataset where we try to predict the 'label' giving a sample weight 'weight' using a set of features. The problem consists of a set of very sparse timeseries, the features are used to give context to these timeseries. With the classification we are trying to predict the label in the feature, i.e. the validation set has values for 't' which are all unseen in during training (as training happens before validation in time). We couldn't find any literature how to deal with this problem (i.e. thousands of timeseries with a lot of missing data, trying to use contextual features to still be able to predict).\n",
    "\n",
    "The dataset contains the following features:\n",
    "\n",
    "- label: the label as a binary classification, either 0 or 1\n",
    "- weight: the sample weight\n",
    "- feature_0-8: categorical features\n",
    "- t(_squared/_cubed): time component, first day of training data is t=0 etc, will be non-overlapping between training and validation set\n",
    "- best_guess(_2): best guess for the probability of the positive class\n",
    "\n",
    "We try to interact time and the best_guess feature with the categorical features in the hope of gaining an improvement. The 'best_guess' is also the model that should be beaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"training_data_sample.csv\").dropna()\n",
    "validate_df = pd.read_csv(\"validation_data_sample.csv\").dropna()\n",
    "test_df = pd.read_csv(\"test_data_sample.csv\").dropna()\n",
    "\n",
    "train_df = train_df[train_df.weight>0]#.sample(frac=0.1)\n",
    "validate_df = validate_df[validate_df.weight>0]#.sample(frac=0.1)\n",
    "test_df = test_df[test_df.weight>0]#.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['label']\n",
    "WEIGHT = ['weight']\n",
    "\n",
    "FEATURES_CAT = [\n",
    "    #'feature_0'\n",
    "    #,'feature_1'\n",
    "    #,'feature_2'\n",
    "    'feature_3'\n",
    "    ,'feature_4'\n",
    "    ,'feature_5'\n",
    "    ,'feature_6'\n",
    "    ,'feature_7'\n",
    "    ,'feature_8'\n",
    "]\n",
    "\n",
    "FEATURES_CONT = [\n",
    "   't','t_squared','t_cubed','best_guess_2'\n",
    "]\n",
    "\n",
    "FEATURES_CROSSED = [\n",
    "    #('t','feature_0')\n",
    "    #, ('t','feature_1')\n",
    "    #, ('t','feature_2')\n",
    "     ('t','feature_3')\n",
    "    , ('t','feature_4')\n",
    "    , ('t','feature_5')\n",
    "    , ('t','feature_6')\n",
    "    , ('t','feature_7')\n",
    "    \n",
    "    #, ('feature_8','feature_0')\n",
    "    #, ('feature_8','feature_1')\n",
    "    #, ('feature_8','feature_2')\n",
    "    , ('feature_8','feature_3')\n",
    "    , ('feature_8','feature_4')\n",
    "    , ('feature_8','feature_5')\n",
    "    , ('feature_8','feature_6')\n",
    "    , ('feature_8','feature_7')\n",
    "    \n",
    "    #, ('best_guess_2','feature_0')\n",
    "    #, ('best_guess_2','feature_1')\n",
    "    #, ('best_guess_2','feature_2')\n",
    "    , ('best_guess_2','feature_3')\n",
    "    , ('best_guess_2','feature_4')\n",
    "    , ('best_guess_2','feature_5')\n",
    "    , ('best_guess_2','feature_6')\n",
    "    , ('best_guess_2','feature_7')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred,weight,label):\n",
    "    # Tensorflow reports logloss with a different denominator - adding both for consistency\n",
    "    return pd.Series(index=['logloss', 'logloss_tf']\n",
    "                        ,data=[\n",
    "                            - np.sum( weight * (label * np.log(pred) + (1 - label) * np.log(1 - pred)) ) / np.sum(weight),\n",
    "                        - np.sum( weight * (label * np.log(pred) + (1 - label) * np.log(1 - pred)) ) / len(weight)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  feature_3 , voc size:  759\n",
      "Feature:  feature_4 , voc size:  65\n",
      "Feature:  feature_5 , voc size:  184\n",
      "Feature:  feature_6 , voc size:  12\n",
      "Feature:  feature_7 , voc size:  40\n",
      "Feature:  feature_8 , voc size:  7\n",
      "Feature:  t  +  feature_3\n",
      "Feature:  t  +  feature_4\n",
      "Feature:  t  +  feature_5\n",
      "Feature:  t  +  feature_6\n",
      "Feature:  t  +  feature_7\n",
      "Feature:  feature_8  +  feature_3\n",
      "Feature:  feature_8  +  feature_4\n",
      "Feature:  feature_8  +  feature_5\n",
      "Feature:  feature_8  +  feature_6\n",
      "Feature:  feature_8  +  feature_7\n",
      "Feature:  best_guess_2  +  feature_3\n",
      "Feature:  best_guess_2  +  feature_4\n",
      "Feature:  best_guess_2  +  feature_5\n",
      "Feature:  best_guess_2  +  feature_6\n",
      "Feature:  best_guess_2  +  feature_7\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "for feature_name in FEATURES_CAT:\n",
    "    \n",
    "    train_df[feature_name] = train_df[feature_name].astype(str)\n",
    "    validate_df[feature_name] = validate_df[feature_name].astype(str)\n",
    "    test_df[feature_name] = test_df[feature_name].astype(str)\n",
    "\n",
    "    vocabulary = pd.concat([train_df[feature_name],validate_df[feature_name],test_df[feature_name]]).unique()\n",
    "    print(\"Feature: \", feature_name, \", voc size: \", len(vocabulary))\n",
    "    col = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n",
    "    if feature_name in ['feature_0','feature_1','feature_2']:\n",
    "        feature_columns.append(tf.feature_column.embedding_column(col,dimension=64))\n",
    "    else:\n",
    "        feature_columns.append(tf.feature_column.indicator_column(col))\n",
    "\n",
    "for feature_name in FEATURES_CONT:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name))\n",
    "    \n",
    "for f1,f2 in FEATURES_CROSSED:\n",
    "    print(\"Feature: \", f1, \" + \", f2)\n",
    "    voc2 = pd.concat([train_df[f2],validate_df[f2],test_df[f2]]).unique()\n",
    "    if f1 in FEATURES_CONT:\n",
    "        num_col = tf.feature_column.numeric_column(f1)\n",
    "        if f1=='t':\n",
    "            col = tf.feature_column.bucketized_column(num_col,boundaries=list(np.arange(0.5,101.5,1.0)))\n",
    "        if f1=='best_guess_2':\n",
    "            col = tf.feature_column.bucketized_column(num_col,boundaries=list(np.arange(0,1.01,0.001)))\n",
    "        feature_columns.append(\n",
    "            tf.feature_column.indicator_column(\n",
    "            tf.feature_column.crossed_column(\n",
    "                [col\n",
    "                , tf.feature_column.categorical_column_with_vocabulary_list(f2, voc2)]\n",
    "                , hash_bucket_size=1000)))\n",
    "    elif f1 in FEATURES_CAT:\n",
    "        voc1 = pd.concat([train_df[f1],validate_df[f1],test_df[f1]]).unique()\n",
    "        feature_columns.append(\n",
    "            tf.feature_column.indicator_column(\n",
    "            tf.feature_column.crossed_column(\n",
    "                [tf.feature_column.categorical_column_with_vocabulary_list(f1, voc1)\n",
    "                , tf.feature_column.categorical_column_with_vocabulary_list(f2, voc2)]\n",
    "                , hash_bucket_size=1000)))\n",
    "        \n",
    "train_x = train_df[FEATURES_CAT+FEATURES_CONT]\n",
    "train_weights = train_df[WEIGHT].values.flatten()\n",
    "train_y = train_df[TARGET].values.flatten()\n",
    "\n",
    "validate_x = validate_df[FEATURES_CAT+FEATURES_CONT]\n",
    "validate_weights = validate_df[WEIGHT].values.flatten()\n",
    "validate_y = validate_df[TARGET].values.flatten()\n",
    "\n",
    "test_x = test_df[FEATURES_CAT+FEATURES_CONT]\n",
    "test_weights = test_df[WEIGHT].values.flatten()\n",
    "test_y = test_df[TARGET].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(features, labels, weights, training=True, batch_size=64):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels, weights))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_ds = get_data(train_x, train_y, train_weights, training=True, batch_size=batch_size)\n",
    "val_ds = get_data(validate_x, validate_y, validate_weights, training=False, batch_size=batch_size)\n",
    "test_ds = get_data(test_x, test_y, test_weights, training=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch:  14806.1328125\n"
     ]
    }
   ],
   "source": [
    "print(\"Steps per epoch: \", train_x.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(output_bias=None):\n",
    "    \n",
    "    if output_bias:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.DenseFeatures(feature_columns),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    \n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  -3.4765001309572376\n"
     ]
    }
   ],
   "source": [
    "bias = np.log(np.sum(train_df[train_df.label==1]['weight'])/np.sum(train_df[train_df.label==0]['weight']))\n",
    "print(\"Bias: \",bias)\n",
    "model = make_model(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "Epoch 1/10\n",
      "14807/14807 [==============================] - 913s 62ms/step - loss: 1.6350 - accuracy: 0.8197 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "14806/14807 [============================>.] - ETA: 0s - loss: 1.6191 - accuracy: 0.8197Restoring model weights from the end of the best epoch.\n",
      "14807/14807 [==============================] - 899s 61ms/step - loss: 1.6191 - accuracy: 0.8197 - val_loss: 1.6331 - val_accuracy: 0.8227\n",
      "Epoch 00002: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81bee00390>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=0,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10, shuffle=True, use_multiprocessing=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logloss       0.122638\n",
       "logloss_tf    1.517179\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"BASE:\")\n",
    "compute_metrics(\n",
    "    validate_df.best_guess_2.astype(float)\n",
    "    , validate_df.weight.astype(float)\n",
    "    , validate_df.label.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VARIANT:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logloss       0.131922\n",
       "logloss_tf    1.632031\n",
       "dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"VARIANT:\")\n",
    "compute_metrics(\n",
    "    model.predict(val_ds).flatten()\n",
    "    , validate_df.weight.astype(float)\n",
    "    , validate_df.label.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>best_guess</th>\n",
       "      <th>best_guess_2</th>\n",
       "      <th>t</th>\n",
       "      <th>t_squared</th>\n",
       "      <th>t_cubed</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>0.032436</td>\n",
       "      <td>21</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>4896991224</td>\n",
       "      <td>0</td>\n",
       "      <td>1582895760</td>\n",
       "      <td>5084375</td>\n",
       "      <td>8558615</td>\n",
       "      <td>50300040</td>\n",
       "      <td>783437</td>\n",
       "      <td>4994604</td>\n",
       "      <td>1255224792</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.065101</td>\n",
       "      <td>0.060879</td>\n",
       "      <td>21</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>43023456</td>\n",
       "      <td>0</td>\n",
       "      <td>33297160</td>\n",
       "      <td>10372125</td>\n",
       "      <td>3291775</td>\n",
       "      <td>29341690</td>\n",
       "      <td>2350311</td>\n",
       "      <td>17481114</td>\n",
       "      <td>24993414</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.065565</td>\n",
       "      <td>0.056306</td>\n",
       "      <td>21</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>1717096860</td>\n",
       "      <td>0</td>\n",
       "      <td>60831350</td>\n",
       "      <td>18100375</td>\n",
       "      <td>8558615</td>\n",
       "      <td>31018358</td>\n",
       "      <td>783437</td>\n",
       "      <td>7491906</td>\n",
       "      <td>26844778</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.044676</td>\n",
       "      <td>0.045376</td>\n",
       "      <td>21</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>1090183644</td>\n",
       "      <td>0</td>\n",
       "      <td>260614310</td>\n",
       "      <td>50640375</td>\n",
       "      <td>15142165</td>\n",
       "      <td>47785038</td>\n",
       "      <td>783437</td>\n",
       "      <td>24973020</td>\n",
       "      <td>566980225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.032365</td>\n",
       "      <td>0.032885</td>\n",
       "      <td>21</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>6965958492</td>\n",
       "      <td>0</td>\n",
       "      <td>916312230</td>\n",
       "      <td>14846375</td>\n",
       "      <td>11192035</td>\n",
       "      <td>2515002</td>\n",
       "      <td>2350311</td>\n",
       "      <td>4162170</td>\n",
       "      <td>3164906758</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053079</th>\n",
       "      <td>1053079</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>0.036243</td>\n",
       "      <td>27</td>\n",
       "      <td>729</td>\n",
       "      <td>19683</td>\n",
       "      <td>31201224912</td>\n",
       "      <td>4142250</td>\n",
       "      <td>6754841170</td>\n",
       "      <td>153751500</td>\n",
       "      <td>1975065</td>\n",
       "      <td>104791750</td>\n",
       "      <td>783437</td>\n",
       "      <td>832434</td>\n",
       "      <td>14160620395</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053080</th>\n",
       "      <td>1053080</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.024976</td>\n",
       "      <td>27</td>\n",
       "      <td>729</td>\n",
       "      <td>19683</td>\n",
       "      <td>31077532476</td>\n",
       "      <td>4142250</td>\n",
       "      <td>6742674900</td>\n",
       "      <td>153751500</td>\n",
       "      <td>1975065</td>\n",
       "      <td>51138374</td>\n",
       "      <td>783437</td>\n",
       "      <td>832434</td>\n",
       "      <td>14126370161</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053081</th>\n",
       "      <td>1053081</td>\n",
       "      <td>0.026610</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>27</td>\n",
       "      <td>729</td>\n",
       "      <td>19683</td>\n",
       "      <td>31210444224</td>\n",
       "      <td>4142250</td>\n",
       "      <td>6757402490</td>\n",
       "      <td>153751500</td>\n",
       "      <td>1975065</td>\n",
       "      <td>11736676</td>\n",
       "      <td>4700622</td>\n",
       "      <td>832434</td>\n",
       "      <td>14163397441</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053083</th>\n",
       "      <td>1053083</td>\n",
       "      <td>0.016623</td>\n",
       "      <td>0.031544</td>\n",
       "      <td>27</td>\n",
       "      <td>729</td>\n",
       "      <td>19683</td>\n",
       "      <td>31153591800</td>\n",
       "      <td>4142250</td>\n",
       "      <td>6741394240</td>\n",
       "      <td>153751500</td>\n",
       "      <td>1975065</td>\n",
       "      <td>41916700</td>\n",
       "      <td>783437</td>\n",
       "      <td>832434</td>\n",
       "      <td>281870169</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053085</th>\n",
       "      <td>1053085</td>\n",
       "      <td>0.031490</td>\n",
       "      <td>0.044817</td>\n",
       "      <td>27</td>\n",
       "      <td>729</td>\n",
       "      <td>19683</td>\n",
       "      <td>31125933864</td>\n",
       "      <td>4142250</td>\n",
       "      <td>6741394240</td>\n",
       "      <td>153751500</td>\n",
       "      <td>1975065</td>\n",
       "      <td>41916700</td>\n",
       "      <td>783437</td>\n",
       "      <td>832434</td>\n",
       "      <td>2269309423</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>637695 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  best_guess  best_guess_2   t  t_squared  t_cubed  \\\n",
       "0                 0    0.033504      0.032436  21        441     9261   \n",
       "1                 1    0.065101      0.060879  21        441     9261   \n",
       "2                 2    0.065565      0.056306  21        441     9261   \n",
       "3                 3    0.044676      0.045376  21        441     9261   \n",
       "5                 5    0.032365      0.032885  21        441     9261   \n",
       "...             ...         ...           ...  ..        ...      ...   \n",
       "1053079     1053079    0.031336      0.036243  27        729    19683   \n",
       "1053080     1053080    0.024375      0.024976  27        729    19683   \n",
       "1053081     1053081    0.026610      0.022022  27        729    19683   \n",
       "1053083     1053083    0.016623      0.031544  27        729    19683   \n",
       "1053085     1053085    0.031490      0.044817  27        729    19683   \n",
       "\n",
       "           feature_0 feature_8   feature_2  feature_3 feature_4  feature_5  \\\n",
       "0         4896991224         0  1582895760    5084375   8558615   50300040   \n",
       "1           43023456         0    33297160   10372125   3291775   29341690   \n",
       "2         1717096860         0    60831350   18100375   8558615   31018358   \n",
       "3         1090183644         0   260614310   50640375  15142165   47785038   \n",
       "5         6965958492         0   916312230   14846375  11192035    2515002   \n",
       "...              ...       ...         ...        ...       ...        ...   \n",
       "1053079  31201224912   4142250  6754841170  153751500   1975065  104791750   \n",
       "1053080  31077532476   4142250  6742674900  153751500   1975065   51138374   \n",
       "1053081  31210444224   4142250  6757402490  153751500   1975065   11736676   \n",
       "1053083  31153591800   4142250  6741394240  153751500   1975065   41916700   \n",
       "1053085  31125933864   4142250  6741394240  153751500   1975065   41916700   \n",
       "\n",
       "        feature_6 feature_7    feature_1  label  weight  \n",
       "0          783437   4994604   1255224792      0      12  \n",
       "1         2350311  17481114     24993414      0       1  \n",
       "2          783437   7491906     26844778      0       7  \n",
       "3          783437  24973020    566980225      1       1  \n",
       "5         2350311   4162170   3164906758      0       3  \n",
       "...           ...       ...          ...    ...     ...  \n",
       "1053079    783437    832434  14160620395      0       7  \n",
       "1053080    783437    832434  14126370161      0       4  \n",
       "1053081   4700622    832434  14163397441      0       1  \n",
       "1053083    783437    832434    281870169      0      40  \n",
       "1053085    783437    832434   2269309423      0       5  \n",
       "\n",
       "[637695 rows x 17 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
